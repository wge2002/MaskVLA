import{c as a,a as e,b as t,d as i,F as s,o,e as n}from"./@vue-DNEHza7E.js";!function(){const a=document.createElement("link").relList;if(!(a&&a.supports&&a.supports("modulepreload"))){for(const a of document.querySelectorAll('link[rel="modulepreload"]'))e(a);new MutationObserver(a=>{for(const t of a)if("childList"===t.type)for(const a of t.addedNodes)"LINK"===a.tagName&&"modulepreload"===a.rel&&e(a)}).observe(document,{childList:!0,subtree:!0})}function e(a){if(a.ep)return;a.ep=!0;const e=function(a){const e={};return a.integrity&&(e.integrity=a.integrity),a.referrerPolicy&&(e.referrerPolicy=a.referrerPolicy),"use-credentials"===a.crossOrigin?e.credentials="include":"anonymous"===a.crossOrigin?e.credentials="omit":e.credentials="same-origin",e}(a);fetch(a.href,e)}}();n(((a,e)=>{const t=a.__vccOpts||a;for(const[i,s]of e)t[i]=s;return t})({},[["render",function(n,c){return o(),a(s,null,[c[0]||(c[0]=e("div",{class:"articleTitleContainer contentContainer"},[e("div",{class:"articleTitle"},"MaskVLA:"),e("div",{class:"articleTitle"}," Visual Masking Against Trajectory Overfitting of Vision-Language-Action Model "),e("div",{class:"arthorNameLine"},[e("div",{class:"arthorNameContainer"},[e("span",{class:"arthorName"},"anonymous author 1"),e("span",{class:"sup"},"1"),e("span",{class:"arthorName"},"， ")]),e("div",{class:"arthorNameContainer"},[e("span",{class:"arthorName"},"anonymous author 2"),e("span",{class:"sup"},"1"),e("span",{class:"arthorName"},"， ")]),e("div",{class:"arthorNameContainer"},[e("span",{class:"arthorName"},"anonymous author N"),e("span",{class:"sup"},"N"),e("span",{class:"arthorName"},"， ")])]),e("div",{class:"expression"},[e("div",{class:"expressionItem"},[e("span",{class:"sup"}," * "),e("span",{class:"expressionText"},"Equal Contribution")])]),e("div",{class:"university"},[e("div",{class:"universityContainer"},[e("span",{class:"sup"}," 1 "),e("span",{class:"universityName"},"anonymous organization ")])]),e("div",{class:"buttonGroup"},[e("button",{class:"button",onclick:"window.open('/')",type:"button"},[e("div",{class:"buttonIcon arxivIcon"}),i(" Paper ")]),e("button",{class:"button",onclick:"window.open('/')",type:"button"},[e("div",{class:"buttonIcon gitHubIcon"}),i(" Code ")]),e("button",{class:"button",onclick:"window.open('/')",type:"button"},[e("div",{class:"buttonIcon huggingFaceIcon"}),i(" Model ")])])],-1)),c[1]||(c[1]=e("div",{class:"videoDisplayContainer contentContainer coverVideo"},[e("video",{autoplay:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/intro-BMZcMmW1.mp4",type:"video/mp4"})])],-1)),c[2]||(c[2]=t('<div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentMainTitle" data-v-72a6049a>Abstract</div><div class="content" data-v-72a6049a> Vision-Language-Action (VLA) models integrate vision-language understanding with executable robot actions, enabling end-to-end learning for robot control. However, our empirical analysis reveals that existing models exhibit severe trajectory memorization and overfitting when finetuned on limited datasets. To guide the model in effectively utilizing wrist camera information, we propose MaskVLA, a masking-based fine-tuning strategy. By randomly masking a small portion of the main camera’s visual information, the model is guided to autonomously learn more fine-grained, task-relevant, and effective visual features. This process leads to the emergence of robust policies, thereby enhancing the model’s capability to tackle complex manipulation tasks and improving its generalization performance. Our method has been comprehensively evaluated on RoboTwin 2.0, achieving an average success rate improvement of 22.9% and 16.5% compared to π0 and OpenVLA-OFT, respectively. Furthermore, experiments on real-world ALOHA robots also demonstrate the effectiveness of our approach. Our project page is <a href="https://anonymous.4open.science/w/MaskVLA-D31F/" target="_blank" data-v-72a6049a>https://anonymous.4open.science/w/MaskVLA-D31F/</a>. </div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentMainTitle" data-v-72a6049a>Introduction</div><div class="contentImg mediumImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic1-C4px2jGP.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 1.</span> We propose <span class="black" data-v-72a6049a>MaskVLA</span>, an improved fine-tuning method for multiview VLA frameworks that significantly enhances model performance. By randomly masking the primary camera input, the method guides the model to learn robust multi-view visual features. In both RoboTwin2.0 simulation environments and real-world experiments, it demonstrates substantial improvements over OpenVLA-OFT across multiple manipulation tasks. </div></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentMainTitle" data-v-72a6049a>Methodology</div><div class="contentImg mediumImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic2-xFPIHc7j.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 2.</span><span class="black" data-v-72a6049a> Grad-CAM Analysis:</span> Qualitative comparisons on two RoboTwin tasks. <span class="black" data-v-72a6049a>Row 1:</span> Input images. <span class="black" data-v-72a6049a>Row 2:</span> Baseline (OpenVLA-OFT) shows scattered attention on irrelevant areas. <span class="black" data-v-72a6049a>Row 3:</span> MaskVLA concentrates attention precisely on task-critical regions validating its superior visual grounding via masked training. </div></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic3--FDA16NS.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 3.</span><span class="black" data-v-72a6049a> Architecture of our MaskVLA.</span> During the fine-tuning stage, a small portion of the input from the primary camera (e.g., 10%) is masked out. During inference, the model receives complete multi-camera inputs. This masking strategy reduces the model’s dependency on the primary camera and guides more robust and generalized feature learning. </div></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentMainTitle" data-v-72a6049a>Simulation Experiment</div><div class="contentImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic4-D-3ooH38.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 4.</span><span class="black" data-v-72a6049a> Simulation experimental task demonstration.</span> We selected 4 tasks from the 50 tasks provided by robotwin and tested both their easy and hard modes respectively. (a) &quot;Adjust bottle&quot; easy mode; (b) &quot;Adjust bottle&quot; hard mode; (c) &quot;Open laptop&quot; easy mode; (d) &quot;Open laptop&quot; hard mode; (e) &quot;Put object cabinet&quot; easy mode; (f) &quot;Put object cabinet&quot; hard mode; (g) &quot;Stack bowls three&quot; easy mode; (h) &quot;Stack bowls three&quot; hard mode. </div></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentImg mediumImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic5-CZz5kiok.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 5.</span><span class="black" data-v-72a6049a> Failure Analysis.</span> (a) represents grasping at empty space caused by overfit; (b) represents model decision failure caused by OOD, continuously outputting repetitive action chunks with robotic arms repeatedly retreating; (c) represents situations where manipulated objects are knocked away due to internal object collision code in RoboTwin2.0 (as highlighted by the red box in the figure); (d) represents cases where low-quality task completion is judged as failure by the evaluation program; (e) represents situations other than the main cases mentioned above. The two pie charts on the right represent the quantity and proportion of failure cases for baseline and MaskVLA in evaluation respectively. </div></div></div>',6)),c[3]||(c[3]=e("div",{class:"videoDisplayGroupContainer contentContainer simVideo"},[e("div",{class:"occupied"}),e("div",{class:"resultType"},"Ours✅️"),e("div",{class:"resultType"},"Baseline❌"),e("div",{class:"occupied"}),e("div",{class:"difficultyType"},"Easy"),e("div",{class:"difficultyType"},"Hard"),e("div",{class:"difficultyType"},"Easy"),e("div",{class:"difficultyType"},"Hard"),e("div",{class:"taskType"},[e("div",{class:"taskName"},"Adjust Bottle"),e("div",{class:"taskName"},"Open Laptop"),e("div",{class:"taskName"},"Put Object Cabinet"),e("div",{class:"taskName"},"Stack Bowls Three")]),e("div",{class:"successCaseContainer"},[e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_adjust_clean_success_5x-Ibzaotqe.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_adjust_randomized_success_5x-CU0HIO59.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_openlaptop_clean_success-B0xcF85A.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_openlaptop_randomized_success-BfvLuITc.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_putobject_clean_success-eTqqtJDu.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_putobject_randomized_success-C5_9vZga.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_stack_clean_success-DjGoJZQg.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_stack_randomized_success-CqWJ29CK.mp4",type:"video/mp4"})])])]),e("div",{class:"failureCaseContainer"},[e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_adjust_clean_failed_5x-BEFpZFiK.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_adjust_randomized_failed_5x-Bns9NPkN.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_openlaptop_clean_failed-Ct5kUnNi.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_openlaptop_randomized_failed-tNkjwFoL.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_putobject_clean_failed-Bca3i84n.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_putobject_randomized_failed-Ck07yy9D.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_stack_clean_failed-I3pVb2Qw.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/sim_stack_randomized_failed-q73Bw5Z0.mp4",type:"video/mp4"})])])])],-1)),c[4]||(c[4]=t('<div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic6-B9xP2chj.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 6. </span> ALOHA robot experimental task demonstration. Four representative tasks test MaskVLA’s multi-view information integration capability: (a) &quot;Feed carrot&quot; - Place the carrot in the food plate; (b) &quot;Adjust cups&quot; - Straighten the cups and place them on the plate; (c) &quot;Place orange&quot; - Place the orange in the fruit dish; (d) &quot;Stack bowls&quot; - Stack the bowls in sequence. </div></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentImg mediumImg" data-v-72a6049a><img src="/MaskVLA/static/png/table2-v3_u_pZj.png" alt="image" data-v-72a6049a></div></div><div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentMainTitle" data-v-72a6049a>Real World Experiment</div></div>',3)),c[5]||(c[5]=e("div",{class:"videoDisplayGroupContainer contentContainer realWorldVideo"},[e("div",{class:"coverVideo"},[e("div",{class:"introText"},"Feed Carrot"),e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/real_carrot_success-D6m1q-k1.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("div",{class:"introText"},"Adjust Cups"),e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/real_adjust_cup_success-CyAE2Zpq.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("div",{class:"introText"},"Place Orange"),e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/real_orange_success-Byf0orn_.mp4",type:"video/mp4"})])]),e("div",{class:"coverVideo"},[e("div",{class:"introText"},"Stack Bowls"),e("video",{autoplay:"",muted:"",playsinline:"",loop:"",controls:"","disable-picture-in-picture":"true"},[e("source",{src:"/MaskVLA/static/mp4/real_stack_right_success-CunIJI_9.mp4",type:"video/mp4"})])])],-1)),c[6]||(c[6]=t('<div class="articleMainBodyContainer contentContainer" data-v-72a6049a><div class="contentImg tinyImg" data-v-72a6049a><img src="/MaskVLA/static/png/pic7-WL4N_chl.png" alt="image" data-v-72a6049a><div class="introText" data-v-72a6049a><span class="bold" data-v-72a6049a>Fig. 7. </span> Real-world multi-task results. </div></div></div>',1))],64)}],["__scopeId","data-v-72a6049a"]])).mount("#app");
